---
title: "failure-randomforest"
author: "Doug McNeall"
date: "9/14/2023"
output: html_document
---


Using a randomForest to find lowcarbon and crash locations in the JULES ensemble. 

Useful tutorial for random forests in R:  
https://www.r-bloggers.com/2021/04/random-forest-in-r/

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

set.seed(42)

library(RColorBrewer)
library(e1071)
library(MASS)
library(caret)

library(randomForest)

```

```{r}
load('data/wave0_summary.RData')

```

## Plot the data

```{r, fig.width = 12, fig.height = 12}

d <- ncol(X)
lowcarbon_ix <- which(Y_char =='LOWCARBON')
crashed_ix <- which(Y_char == 'CRASHED')

x1 <- X[lowcarbon_ix , ]
x2 <- X[crashed_ix ,]

XY <- rbind(x1, x2)

pairs(XY,
      lower.panel=function(x, y, ...) {
        Xx <- x[seq_len(nrow(x1))] 
        Xy <- y[seq_len(nrow(x1))] 

        points(Xx, Xy, col = 'blue', pch = 19, cex = 0.8)
      }, 
      upper.panel=function(x, y, ...) {
        Yx <- x[(nrow(x1) + 1):length(x)]
        Yy <- y[(nrow(x1) + 1):length(y)] 
        

        points(Yx, Yy, col = 'red', pch = 19, cex = 0.8)

      }, 
      gap = 0,
      xlim = c(0,1), ylim = c(0,1),
      labels = 1:d,
      oma = c(2, 18, 2, 2)) # move the default tick labels off the plot 


reset <- function()
  {
  # Allows annotation of graphs, resets axes
  par(mfrow=c(1, 1), oma=rep(0, 4), mar=rep(0, 4), new=TRUE)
  plot(0:1, 0:1, type="n", xlab="", ylab="", axes=FALSE)
}
reset()

legend('left', legend = paste(1:d, colnames(lhs)), cex = 1.1, bty = 'n')
legend('topleft', pch = 19, col = c('red', 'blue'), legend = c('CRASHED', 'LOWCARBON'), bty = 'n', inset = 0.02, cex = 1.1 )

```


```{r}


data_df <- data.frame(X, y = as.factor(Y_char))

ix_train <- 1:399
ix_test <- 400:499

train <- data_df[ix_train, ]
test <- data_df[ix_test, ]

```


```{r}

rf <- randomForest(y~., data=train, proximity=TRUE)
print(rf)

```


## Confusion matrix for the training set
```{r}
p1 <- predict(rf, train)
confusionMatrix(p1, train$y)
```

## Confusion matrix for the test set

```{r}

p2 <- predict(rf, test)
confusionMatrix(p2, test$y)

```
```{r}

p3 <- predict(rf, test, type = 'prob')

```


```{r}

mat_unif <- function(nr, nc, cn = NULL){
  
  nsamp <- nc*nr
  samp <- runif(nsamp)
  out <- matrix(data = samp, nrow = nr, ncol = nc)
  colnames(out) <- cn
  out
}

xpred <- mat_unif(nr = 500, nc = ncol(X), cn = colnames(X))


```

```{r}

p4 <- predict(rf, xpred, type = 'prob')


```

```{r}
library(viridis)


cols <- viridis(10)[as.numeric(cut(p4[,2],breaks = 10))]

#plot(dat$x,dat$y,pch = 20,col = dat$Col)
plot(xpred[,4 ], xpred[,15], col = cols, pch = 19)

```


```{r, fig.width = 12, fig.height = 12}

pairs(xpred,
      gap = 0.2,
      col = cols,
      pch = 19,
      cex = 0.5,
      lower.panel = NULL,
      labels = 1:d,
      xaxt = 'n', yaxt = 'n'
      )

reset()
legend('left', legend = paste(1:d, colnames(lhs)), cex = 1.1, bty = 'n')

library(fields)

image.plot(legend.only = TRUE,
           zlim = c(0,1),
           col = viridis(10),
           legend.args = list(text = 'Probability of model failure due to LOWCARBON', side = 3, line = 1),
           legend.shrink = 0.6,
           horizontal = TRUE)




```



```{r}
plot(rf)
```


```{r}
t <- tuneRF(train[,-5], train[,5],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 150,
       trace = TRUE,
       improve = 0.05)

```


```{r}
hist(treesize(rf),
     main = "No. of Nodes for the Trees",
     col = "grey")

```

## Variable importance plot clearly identifies the two most important parameters

```{r}
#Variable Importance
varImpPlot(rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")

```


```{r}
importance(rf)
#MeanDecreaseGini
```

## Partial importance of the two most important parameters

```{r}

partialPlot(rf, train, f0_io)

```

```{r}

partialPlot(rf, train, b_wl_io)

```


```{r}

MDSplot(rf, train$y)
```


## Sample from the whole input space and make predictions

```{r}
samp <- runif(16000)

samp_mat <- matrix(data = samp, nrow = 500)

colnames(samp_mat) <- colnames(X)

samp_pred <- predict(rf, samp_mat)

```


```{r, fig.width = 7, fig.height = 7}

pal = c('red', 'blue','grey')
plot(samp_mat[,4], samp_mat[,8], col = pal[samp_pred], xlab = colnames(X)[4], ylab = colnames(X)[8])

```

## The pairs plot looks very similar to the test data set at the start of the page
```{r, fig.width = 12, fig.height = 12}

pairs(
  samp_mat[samp_pred=='LOWCARBON', ],
  gap = 0,
  col = 'blue',
  upper.panel = NULL,
  pch = 20,
  labels = 1:d
  
)

reset()

legend('right', legend = paste(1:d, colnames(lhs)), cex = 1.1, bty = 'n')
legend('topright', pch = 19, col = c('red', 'blue'), legend = c('failed', 'zero carbon cycle'), bty = 'n', inset = 0.02, cex = 1.1 )


```
## Density of lowcarbon

```{r}



```

# How does classifier accuracy change with ensemble size? 

```{r}

# This function does a simple bootstrap test of the randomForest algorithm
# for a set number of ensemble members (ntrain). It splits the data into a training
# set of ntrain rows and a test set of ntest rows, trains the model, predicts the
# test set and records the misclassification rate for each rep in the oputput.

boot_rf <- function(data, ntrain, ntest, nreps){
  
  outvec <- rep(NA, nreps)
    
  for(i in 1:nreps){
    
    n_train_and_test <- ntrain+ntest
    
    all_samp_ix <- sample(1:nrow(X), n_train_and_test)
    train_ix <- all_samp_ix[1:ntrain]
    test_ix <- all_samp_ix[(ntrain+1):n_train_and_test]
    
    train <- data[train_ix, ]
    test  <- data[test_ix ,]
    
    
    rf <- randomForest(y~., data=train, proximity=TRUE)
    
      
    pred_test <- predict(rf, test)
    conf_test <- confusionMatrix(pred_test, test$y)
    
    out <- (1 - conf_test$overall['Accuracy'])
    outvec[i] <- out
    
  }
  
  outvec
  
}


```


```{r}
# test the above
test_boot <- boot_rf(data = data_df, ntrain = 100, ntest = 10, nreps = 10)

```

## How does misclassification rate vary with ensemble size?

```{r}

nreps <- 50

nens_vec <- seq(from = 100, to = 400, by = 20)

outmat <- matrix(nrow = nreps, ncol = length(nens_vec))

for(j in 1:length(nens_vec)){
  
  boot_rf_out  <- boot_rf(data = data_df, ntrain = nens_vec[j], ntest = 50, nreps = nreps)
  
  outmat[, j] <- boot_rf_out
  
}


```

```{r}

# 1- accuracy mean
oma_mean <- apply(outmat,2,mean)

plot(nens_vec, oma_mean, type = 'b' )


```

```{r}


boxplot( outmat)
```


