---
title: "failure-svm"
author: "Doug McNeall"
date: "9/8/2023"
output: html_document
---

This script build a Support Vector Machine that predicts run status of the JULES-ES-1.0 ensemble. Can you make a better classifier?

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

set.seed(42)

library(RColorBrewer)
library(e1071)
library(MASS)
library(caret)




```



```{r}
load('data/wave0_summary.RData')

```

## Visualise the data

There are 3 classes - "RAN" (JULES ran ok), "CRASHED" (JULES crashed, or did not run) and "LOWCARBON" (there is no functioning carbon cycle, though the model ran).  

There are 499 ensemble members, run in a latin hypercube. lhs is raw, X is normalized [0-1].  
Here is a plot of members that are "RAN" or "LOWCARBON"

```{r, fig.width = 12, fig.height = 12}

d <- ncol(X)
lowcarbon_ix <- which(Y_char =='LOWCARBON')
crashed_ix <- which(Y_char == 'CRASHED')

x1 <- X[lowcarbon_ix , ]
x2 <- X[crashed_ix ,]

XY <- rbind(x1, x2)

pairs(XY,
      lower.panel=function(x, y, ...) {
        Xx <- x[seq_len(nrow(x1))] 
        Xy <- y[seq_len(nrow(x1))] 

        points(Xx, Xy, col = 'blue', pch = 19, cex = 0.8)
      }, 
      upper.panel=function(x, y, ...) {
        Yx <- x[(nrow(x1) + 1):length(x)]
        Yy <- y[(nrow(x1) + 1):length(y)] 
        

        points(Yx, Yy, col = 'red', pch = 19, cex = 0.8)

      }, 
      gap = 0,
      xlim = c(0,1), ylim = c(0,1),
      labels = 1:d,
      oma = c(2, 18, 2, 2)) # move the default tick labels off the plot 


reset <- function()
  {
  # Allows annotation of graphs, resets axes
  par(mfrow=c(1, 1), oma=rep(0, 4), mar=rep(0, 4), new=TRUE)
  plot(0:1, 0:1, type="n", xlab="", ylab="", axes=FALSE)
}
reset()

legend('left', legend = paste(1:d, colnames(lhs)), cex = 1.1, bty = 'n')
legend('topleft', pch = 19, col = c('red', 'blue'), legend = c('CRASHED', 'LOWCARBON'), bty = 'n', inset = 0.02, cex = 1.1 )

```



## Split the sample into a training and test set.

```{r}
ix_train <- 1:399
ix_test <- 400:499

train <- cbind(wave0_summary_df[ix_train, 1:32], wave0_summary_df[ix_train, 'Y_char'] )
test <- cbind(wave0_summary_df[ix_test, 1:32], wave0_summary_df[ix_test, 'Y_char'] )


colnames(train) <- c(colnames(wave0_summary_df)[1:32], 'run_status')
colnames(test) <- c(colnames(wave0_summary_df)[1:32], 'run_status')

```


## Simple non-probabilistic SVM classifier, using two known-important inputs

The classifier is much worse if we include all the inputs.

```{r}
# Which inputs to select for classification training?

# Using all inputs is not good.
traincols <- 1:32
train_x <- train[, traincols]
train_y <- train[, 33]

class(train_x) <- 'numeric'

test_x <- test[, traincols]
test_y <- test[, 33]

class(test_x) <- 'numeric'
```


```{r}
svm_fit <- svm(train_x, as.factor(train_y), kerneal = "radial", probability = FALSE)
svm_pred <- predict(svm_fit, newdata=test_x, probability = FALSE)


```


```{r}
# Evaluate the model

confusionMatrix(svm_pred, as.factor(test_y))

```


```{r}
# Which inputs to select for classification training?
# Using two inputs we know to be important gives better results
traincols <- c(8,4)

train_x <- train[, traincols]
train_y <- train[, 33]

class(train_x) <- 'numeric'

test_x <- test[, traincols]
test_y <- test[, 33]

class(test_x) <- 'numeric'
```

class.weights = 'inverse' doesn't work
```{r}

wts <- c(5,2,1)
names(wts) <-  c('CRASHED', 'LOWCARBON', 'RAN')
svm_fit <- svm(train_x, as.factor(train_y), kernel = "radial", class.weights = wts, probability = FALSE)
svm_pred <- predict(svm_fit, newdata=test_x, probability = FALSE)


```


```{r}
# Evaluate the model

confusionMatrix(svm_pred, as.factor(test_y))

```

## Plot the non-probabilistic SVM.

We don't predict any crashes, which might need looking at. Accuracy is 0.84, which feels like it might be improved.
```{r, fig.width = 7, fig.height = 7}

upper <- apply(train_x, 2, max)
lower <- apply(train_x, 2, min)

x1_seq <- seq(from=lower[1], to=upper[1], length.out = 50)
x2_seq <- seq(from=lower[2], to=upper[2], length.out = 50)

# A grid of test points that covers the entire data space
test_grid <- as.matrix(expand.grid(x1_seq, x2_seq))
colnames(test_grid) <- colnames(train_x)

svm_pred_grid <- predict(svm_fit, newdata=test_grid, probability = FALSE)


pal <- c('red', 'blue','grey')
par(las = 1, cex = 1.3)
plot(test_grid, col = pal[svm_pred_grid], pch = 20, cex = 0.8, main = 'SVM classifier prediction')

points(train_x, bg = pal[as.factor(train_y)], cex = 1, pch =21)

legend('topleft', legend = unique(as.factor(train_y)),
pt.bg = pal[unique(as.factor(train_y))],
pch = 21,
col = 'black',
text.col= pal[unique(as.factor(train_y))],
 bg = 'white',
cex = 0.8)
```


## Probabilistic classifier

```{r}
svm_fit_prob <- svm(train_x, as.factor(train_y), probability = TRUE)
svm_pred_prob <- predict(svm_fit_prob, newdata=test_grid, probability = TRUE)
#confusionMatrix(svm_pred, as.factor(test_y))
```


```{r}


svm_pred_prob_df <- attr(svm_pred_prob,'prob')

ranmat <- matrix(svm_pred_prob_df[,'RAN'], nrow = length(x1_seq))

lowcarbonmat <- matrix(svm_pred_prob_df[,'LOWCARBON'], nrow = length(x1_seq))
crashedmat <- matrix(svm_pred_prob_df[,'CRASHED'], nrow = length(x1_seq))

```


```{r, fig.width = 7, fig.height = 7}

pal <- c('red', 'blue','grey')
par(las = 1, cex = 1.3)
plot(test_grid, type = 'n', cex = 0.8, main = 'SVM classifier prediction')


contour(x = x1_seq, y = x2_seq, ranmat,
    add = TRUE, levels = c(0.2, 0.4,0.6,0.8,1))

plot(test_grid, type = 'n', cex = 0.8, main = 'SVM classifier prediction')

contour(x1_seq, x2_seq, lowcarbonmat,
    add = TRUE, levels = c(0.2,0.4,0.6,0.8,1), col =pal[2])

plot(test_grid, type = 'n', cex = 0.8, main = 'SVM classifier prediction')

contour(x1_seq, x2_seq, crashedmat,
    add = TRUE,levels = c(0.5,0.6,0.7,0.8), col = pal[3])
```

```{r, fig.width = 7, fig.height = 7}

pal <- c('red', 'blue','grey')
par(las = 1, cex = 1.3)
#plot(test_grid, type = 'n', cex = 0.8, main = 'SVM classifier prediction')


filled.contour(x = x1_seq, y = x2_seq, ranmat, levels = seq(from = 0, to = 1, by = 0.2), main = "RAN")

#plot(test_grid, type = 'n', cex = 0.8, main = 'SVM classifier prediction')

filled.contour(x = x1_seq, y = x2_seq, lowcarbonmat,levels = seq(from = 0, to = 1, by = 0.2), main = "LOWCARBON")


#plot(test_grid, type = 'n', cex = 0.8, main = 'SVM classifier prediction')

#contour(x1_seq, x2_seq, crashedmat,
#    add = TRUE,levels = c(0.5,0.6,0.7,0.8), col = pal[3])
```

